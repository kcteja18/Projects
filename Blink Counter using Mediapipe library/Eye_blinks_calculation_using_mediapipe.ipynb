{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617272ff",
   "metadata": {},
   "source": [
    "#  EYE TRACKING AND BLINK RATIO CALCULATION using MediaPipe model and OpenCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f591569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import time\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f740f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# variables...\n",
    "frame_counter = 0\n",
    "cef_counter = 0\n",
    "total_blinks = 0\n",
    "\n",
    "BLACK = (0,0,0)\n",
    "WHITE = (255,255,255)\n",
    "BLUE = (255,0,0)\n",
    "RED = (0,0,255)\n",
    "CYAN = (255,255,0)\n",
    "YELLOW =(0,255,255)\n",
    "MAGENTA = (255,0,255)\n",
    "GRAY = (128,128,128)\n",
    "GREEN = (0,255,0)\n",
    "PURPLE = (128,0,128)\n",
    "ORANGE = (0,165,255)\n",
    "PINK = (147,20,255)\n",
    "\n",
    "# face bounder indices \n",
    "# FACE_OVAL=[ 10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103,67, 109]\n",
    "\n",
    "# lips indices for Landmarks\n",
    "LIPS=[ 61, 146, 91, 181, 84, 17, 314, 405, 321, 375,291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95,185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78 ]\n",
    "LOWER_LIPS =[61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\n",
    "UPPER_LIPS=[ 185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78] \n",
    "\n",
    "# Left eyes indices \n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
    "\n",
    "# right eyes indices\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]  \n",
    "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "cam = cv2.VideoCapture(\"D:/Emotion_recognition/archive/emotion.mp4\")\n",
    "\n",
    "# function for landmarks detection... \n",
    "def LandmarksDetection(image,results,draw=False):\n",
    "    img_h, img_w,_ = image.shape\n",
    "    # for point in results.multi_face_landmarks:\n",
    "    #     mesh_points = [(int(point.x * img_w),int(point.y * img_h))]\n",
    "    mesh_points = [(int(point.x * img_w), int(point.y * img_h)) for point in results.multi_face_landmarks[0].landmark]\n",
    "    if draw:\n",
    "        [cv2.circle(image, p, 2, (0,255,0), -1) for p in mesh_points]\n",
    "        \n",
    "    return mesh_points\n",
    "# function for calculating eucleadian distance...\n",
    "def eucldian_distance(X,Y):\n",
    "    x1,y1 = X\n",
    "    x2,y2 = Y\n",
    "    dis = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    return dis\n",
    "# function for calculation of blinking ratio...\n",
    "def blinkratio(image,landmarks,right_indices,left_indices):\n",
    "    # draw horizontal line on right eye\n",
    "    right_rh = landmarks[right_indices[0]]\n",
    "    left_rh = landmarks[right_indices[8]]\n",
    "    # draw vertical line on right eye\n",
    "    top_rv = landmarks[right_indices[12]]\n",
    "    bottom_rv = landmarks[right_indices[4]]\n",
    "    # draw horizontal line on left eye\n",
    "    right_lh = landmarks[left_indices[0]]\n",
    "    left_lh = landmarks[left_indices[8]]\n",
    "    # draw vertical line on left eye\n",
    "    top_lv = landmarks[left_indices[12]]\n",
    "    bottom_lv = landmarks[left_indices[4]]\n",
    "    \n",
    "    right_h_dis = eucldian_distance(right_rh,left_rh)\n",
    "    right_v_dis = eucldian_distance(top_rv,bottom_rv)\n",
    "    \n",
    "    left_h_dis = eucldian_distance(right_lh,left_lh)\n",
    "    left_v_dis = eucldian_distance(top_lv,bottom_lv)\n",
    "    \n",
    "    right_eye_ratio = right_h_dis/right_v_dis\n",
    "    left_eye_ratio = left_h_dis/left_v_dis\n",
    "    \n",
    "    blink_ratio = (right_eye_ratio+left_eye_ratio)/2\n",
    "    \n",
    "    return blink_ratio\n",
    "\n",
    "def eyesExtractor(image,r_eye_points,l_eye_points):\n",
    "    # converting color image to gray scale image\n",
    "    gray_img = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    img_size = gray_img.shape\n",
    "    mask = np.zeros(img_size,dtype=np.uint8)\n",
    "    cv2.fillPoly(mask,[np.array(r_eye_points,dtype=np.int32)],255)\n",
    "    cv2.fillPoly(mask,[np.array(l_eye_points,dtype=np.int32)],255)\n",
    "    \n",
    "    eyes = cv2.bitwise_and(gray_img,gray_img,mask=mask)\n",
    "    eyes[mask==0]=155\n",
    "    r_max_x = (max(r_eye_points, key=lambda item: item[0]))[0]\n",
    "    r_min_x = (min(r_eye_points, key=lambda item: item[0]))[0]\n",
    "    r_max_y = (max(r_eye_points, key=lambda item : item[1]))[1]\n",
    "    r_min_y = (min(r_eye_points, key=lambda item: item[1]))[1]\n",
    "    \n",
    "    \n",
    "    l_max_x = (max(l_eye_points, key=lambda item: item[0]))[0]\n",
    "    l_min_x = (min(l_eye_points, key=lambda item: item[0]))[0]\n",
    "    l_max_y = (max(l_eye_points, key=lambda item : item[1]))[1]\n",
    "    l_min_y = (min(l_eye_points, key=lambda item: item[1]))[1]\n",
    "\n",
    "    # croping the eyes from mask \n",
    "    cropped_right = eyes[r_min_y: r_max_y, r_min_x: r_max_x]\n",
    "    cropped_left = eyes[l_min_y: l_max_y, l_min_x: l_max_x]\n",
    "\n",
    "    # returning the cropped eyes \n",
    "    return cropped_right, cropped_left\n",
    "\n",
    "def EyePositionEstimator(eye):\n",
    "    h,w = eye.shape\n",
    "    guass_blur = cv2.GaussianBlur(eye,(9,9),0)\n",
    "    median_blur = cv2.medianBlur(guass_blur,3)\n",
    "    \n",
    "    ret, threshed_eye = cv2.threshold(median_blur,130,255,cv2.THRESH_BINARY)\n",
    "    fxd_part = int(w/3)\n",
    "    \n",
    "    right_eye_part = threshed_eye[0:h,0:fxd_part]\n",
    "    center_eye_part = threshed_eye[0:h,fxd_part:fxd_part+fxd_part]\n",
    "    left_eye_part = threshed_eye[0:h,fxd_part+fxd_part:w]\n",
    "    \n",
    "    eye_position, color = pixelCounter(right_eye_part,center_eye_part,left_eye_part)\n",
    "    return eye_position, color\n",
    "\n",
    "def pixelCounter(first_part,second_part,third_part):\n",
    "    right_eye_part = np.sum(first_part==0)\n",
    "    center_eye_part = np.sum(second_part==0)\n",
    "    left_eye_part = np.sum(third_part==0)\n",
    "    \n",
    "    eye_parts = [right_eye_part,center_eye_part,left_eye_part]\n",
    "    \n",
    "    max_ind = eye_parts.index(max(eye_parts))\n",
    "    pos_eye = \"\"\n",
    "    if max_ind ==0:\n",
    "        pos_eye = \"RIGHT\"\n",
    "        color = [BLACK,GREEN]\n",
    "    elif max_ind == 1:\n",
    "        pos_eye = \"CENTER\"\n",
    "        color = [YELLOW,PINK]\n",
    "    elif max_ind == 2:\n",
    "        pos_eye = \"LEFT\"\n",
    "        color = [GRAY,YELLOW]\n",
    "    else:\n",
    "        pos_eye = \"CLOSED\"\n",
    "        color = [GRAY,YELLOW]\n",
    "    return pos_eye, color\n",
    "\n",
    "def textWithBackground(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0), bgColor=(0,0,0), pad_x=3, pad_y=3, bgOpacity=0.5):\n",
    "    (t_w, t_h), _= cv2.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
    "    x, y = textPos\n",
    "    overlay = img.copy() # coping the image\n",
    "    cv2.rectangle(overlay, (x-pad_x, y+ pad_y), (x+t_w+pad_x, y-t_h-pad_y), bgColor,-1) # draw rectangle \n",
    "    new_img = cv2.addWeighted(overlay, bgOpacity, img, 1 - bgOpacity, 0) # overlaying the rectangle on the image.\n",
    "    cv2.putText(new_img,text, textPos,font, fontScale, textColor,textThickness ) # draw in text\n",
    "    img = new_img\n",
    "\n",
    "    return img\n",
    "\n",
    "def colorBackgroundText(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0), bgColor=(0,0,0), pad_x=3, pad_y=3):\n",
    "    (t_w, t_h), _= cv2.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
    "    x, y = textPos\n",
    "    cv2.rectangle(img, (x-pad_x, y+ pad_y), (x+t_w+pad_x, y-t_h-pad_y), bgColor,-1) # draw rectangle \n",
    "    cv2.putText(img,text, textPos,font, fontScale, textColor,textThickness ) # draw in text\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "CLOSED_EYES_FRAME =3\n",
    "FONTS =cv2.FONT_HERSHEY_COMPLEX\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5,min_tracking_confidence=0.5,max_num_faces=4) as face_mesh:\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        frame_counter += 1\n",
    "        ret,frame = cam.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame,None,fx = 1.5, fy=1.5,interpolation=cv2.INTER_CUBIC)\n",
    "        frame_h, frame_w = frame.shape[:2]\n",
    "        rgb_img = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_img)\n",
    "        if results.multi_face_landmarks:\n",
    "            mesh_points = LandmarksDetection(rgb_img,results,False)\n",
    "            ratio = blinkratio(rgb_img,mesh_points,RIGHT_EYE,LEFT_EYE)\n",
    "            colorBackgroundText(rgb_img,  f'Ratio : {round(ratio,2)}', FONTS, 0.7, (30,100),2, PINK, YELLOW)\n",
    "            \n",
    "            if ratio > 5.5:\n",
    "                cef_counter += 1\n",
    "                colorBackgroundText(rgb_img,  f'Blink', FONTS, 1.7, (int(frame_h/2), 100), 2, YELLOW, pad_x=6, pad_y=6, )\n",
    "                \n",
    "            else:\n",
    "                if cef_counter > CLOSED_EYES_FRAME:\n",
    "                    total_blinks += 1\n",
    "                    cef_counter = 0\n",
    "            colorBackgroundText(rgb_img,  f'Total Blinks: {total_blinks}', FONTS, 0.7, (30,150),2)\n",
    "            cv2.polylines(rgb_img,  [np.array([mesh_points[p] for p in LEFT_EYE ], dtype=np.int32)], True, GREEN, 1, cv2.LINE_AA)\n",
    "            cv2.polylines(rgb_img,  [np.array([mesh_points[p] for p in RIGHT_EYE ], dtype=np.int32)], True, GREEN, 1, cv2.LINE_AA)\n",
    "            cv2.polylines(rgb_img,  [np.array([mesh_points[p]for p in [33,133]], dtype=np.int32)], True, BLUE, 1, cv2.LINE_AA)\n",
    "            cv2.polylines(rgb_img,  [np.array([mesh_points[p]for p in [362,263]], dtype=np.int32)], True, BLUE, 1, cv2.LINE_AA)\n",
    "            cv2.polylines(rgb_img,  [np.array([mesh_points[p]for p in [145,159]], dtype=np.int32)], True, BLUE, 1, cv2.LINE_AA)\n",
    "            cv2.polylines(rgb_img,  [np.array([mesh_points[p]for p in [386,374]], dtype=np.int32)], True, BLUE, 1, cv2.LINE_AA)\n",
    "            # Blink Detector Counter Completed\n",
    "            right_coords = [mesh_points[p] for p in RIGHT_EYE]\n",
    "            left_coords = [mesh_points[p] for p in LEFT_EYE]\n",
    "            crop_right, crop_left = eyesExtractor(rgb_img, right_coords, left_coords)\n",
    "            # cv.imshow('right', crop_right)\n",
    "            # cv.imshow('left', crop_left)\n",
    "            eye_position, color = EyePositionEstimator(crop_right)\n",
    "            colorBackgroundText(rgb_img, f'R: {eye_position}', FONTS, 1.0, (40, 220), 2, color[0], color[1], 8, 8)\n",
    "            eye_position_left, color = EyePositionEstimator(crop_left)\n",
    "            colorBackgroundText(rgb_img, f'L: {eye_position_left}', FONTS, 1.0, (40, 320), 2, color[0], color[1], 8, 8)\n",
    "            \n",
    "        end_time = time.time()-start_time\n",
    "        fps = frame_counter/end_time\n",
    "\n",
    "        frame =textWithBackground(rgb_img,f'FPS: {round(fps,1)}',FONTS, 1.0, (30, 50), bgOpacity=0.9, textThickness=2)\n",
    "        \n",
    "        final_img = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('EYE_TRACKING', final_img)\n",
    "        key = cv2.waitKey(2)\n",
    "        if key==ord('q') or key ==ord('Q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    cam.release()   \n",
    "                    \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9acd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8253c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_holi = mp.solutions.holistic\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# face_mesh = mp_face_mesh.FaceMesh()\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holi.Holistic(min_detection_confidence=0.4,min_tracking_confidence=0.4) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "        img = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        res = holistic.process(img)\n",
    "        # print(res.face_landmarks)\n",
    "        img = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
    "        mp_draw.draw_landmarks(img,res.face_landmarks,mp_holi.FACEMESH_CONTOURS,\n",
    "                               connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_contours_style())\n",
    "        mp_draw.draw_landmarks(img,res.face_landmarks,mp_holi.FACEMESH_TESSELATION,\n",
    "                               connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "        # mp_draw.draw_landmarks(img,res.face_landmarks,mp_holi.FACEMESH_IRISES,\n",
    "        #                        connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_iris_style())\n",
    "        cv2.imshow(\"Facial maps\",img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# # cap3 = cv2.VideoCapture(\"C:/Users/Charanteja/OneDrive/Pictures/Desktop/Emotion_recognition/archive/emotion.mp4\")\n",
    "# cap3 = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     ret,image = cap3.read()\n",
    "#     if ret is not True:\n",
    "#         break\n",
    "#     h,w,_ = image.shape\n",
    "#     # print(\"(Height,width) = \",h,w)\n",
    "#     rgb_img = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "#     res = face_mesh.process(rgb_img)\n",
    "#     for facial_landmarks in res.multi_face_landmarks:\n",
    "#         for i in range(0,468):\n",
    "#             pt1 = facial_landmarks.landmark[i]\n",
    "#             x = int(pt1.x * w)\n",
    "#             y = int(pt1.y * h)\n",
    "#             cv2.circle(image,(x,y),2,(100,100,0),-1)\n",
    "#     cv2.imshow(\"Face_landmarks\",image)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap3.release()\n",
    "# cv2.destroyAllWindows()\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "# mp_draw = mp.solutions.drawing_utils\n",
    "# mp_styles = mp.solutions.drawing_styles\n",
    "# # cap2 = cv2.VideoCapture(\"C:/Users/Charanteja/OneDrive/Pictures/Desktop/Emotion_recognition/archive/emotion.mp4\")\n",
    "# cap2 = cv2.VideoCapture(0)\n",
    "\n",
    "# while cap2.isOpened():\n",
    "#     ret1,frame2 = cap2.read()\n",
    "#     img2 = cv2.cvtColor(frame2,cv2.COLOR_RGB2BGR)\n",
    "#     res2 = mp_face_mesh.FaceMesh(max_num_faces= 5,min_detection_confidence=0.5,min_tracking_confidence=0.5,refine_landmarks=True).process(img2)\n",
    "#     img2 = cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "#     if res2.multi_face_landmarks:\n",
    "#         for fl in res2.multi_face_landmarks:\n",
    "#             mp_draw.draw_landmarks(\n",
    "#                 image=img2,\n",
    "#                 landmark_list = fl,\n",
    "#                 connections = mp_face_mesh.FACEMESH_TESSELATION,\n",
    "#                 landmark_drawing_spec = None,\n",
    "#                 connection_drawing_spec = mp_styles.get_default_face_mesh_tesselation_style()\n",
    "#             )\n",
    "#             # mp_draw.draw_landmarks(\n",
    "#             #     image=img2,\n",
    "#             #     landmark_list = fl,\n",
    "#             #     connections = mp_face_mesh.FACEMESH_CONTOURS,\n",
    "#             #     landmark_drawing_spec = None,\n",
    "#             #     connection_drawing_spec = mp_styles.get_default_face_mesh_contours_style()\n",
    "#             # )\n",
    "#             # mp_draw.draw_landmarks(\n",
    "#             #     image=img2,\n",
    "#             #     landmark_list = fl,\n",
    "#             #     connections = mp_face_mesh.FACEMESH_IRISES,\n",
    "#             #     landmark_drawing_spec = None,\n",
    "#             #     connection_drawing_spec = mp_styles.get_default_face_mesh_iris_style()\n",
    "#             # )\n",
    "#     cv2.imshow(\"Face mesh\",img2)\n",
    "#     if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "# cap2.release()\n",
    "# cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63313a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
